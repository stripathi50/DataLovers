---
title: "US Census Demographic Data"
author: "Data lovers"
date: "06 Dec 2021"
# date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r init, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
library(ezids)
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
```


### Chapter 1:Introduction.

Census is a very important matter, not just at the government level, but for individuals as well. The census population count decides how many congressional members each state will have for the following ten years, as well as how much federal funds localities will receive for roads, schools, housing, and social programs. 

Poverty is a complicated issue. There are numerous elements to it, as well as numerous reasons. The most common definition of poverty focuses on financial hardship. This definition defines poverty rates for towns and countries based on income inequality and financially established poverty lines and quantifies poverty by the amount of money a person makes.

Poverty becomes relative in this way. In the United States, the poverty line for a family of four is little over $26,000 a year. That amount of money is more than 36 times what a family of four living in absolute poverty in a low-income country is forced to subsist on each year.

Many families in the world's poorest countries must make do with less than $1.90 per day to cover their basic requirements. This sort of extreme poverty affects nearly 10% of the world's population.

Understanding poverty solely in terms of economic disparity and calculating relative poverty thresholds makes it impossible to see poverty as anything other than unemployment, bad living conditions, and a low income. Poverty, on the other hand, is a much broader concept.

A fair definition of poverty must take into account the many various aspects of poverty, such as hunger and a lack of shelter, illiteracy and lack of access to education, being sick and unable to see a doctor, and fearing for one's future.

By exploring this data and analyzing it we can come up with many insights, such as how the population varies across the states, which states that have the best transportation, how do income and poverty look like across US, employment/unemployment rates and so many questions that can be answered from these data. 

Therefore, our aim in this project is to analyze the US census of 2015 to answer our interesting question: 


1. Does a relationship exist between ‘child poverty’ and ‘unemployment’? Or how can we predict the unemployment rate?


2. Does the data support ‘professional’ (individual who has a professional job) influence ‘income’?


3. Does a relationship exist between ‘Professional’ and ‘Income’? How can we predict the average income in counties? 



### Chapter 2:Description of the Data.

•	Dataset that is used for our project is “US Census Demographic Data” taken from the common data source Kaggle (which is an online community of data scientists and machine learning practitioners.

Link: https://www.kaggle.com/muonneutrino/us-census-demographic-data?select=acs2015_county_data.csv

•	Dataset contains information about (US Census 2015) American Community Survey 5-year estimates. 

•	Number of variables in the dataset is 37.

•	Number of observations in the dataset is 3220.

•	To perform Exploratory Data Analysis, we primarily focus on the following aspects(variables) from the dataset:

* TotalPop (Total population for county).

*	Income. (Income in county).

*	Poverty (Poverty rate in county).

*	ChildPoverty. (Child poverty rate in county).

* Employed (Total employed for county).

*	Unemployment. (Unemployment rate for county).

* Production. (Rate of this job sector in county).

* WorkAtHome. (Work-at-home rate in county).

* PublicWork. (Public work rate in county).

* SelfEmployed. (Self-employed rate in county).





**This is our dataset before performing any cleansing or dropping columns: **

```{r step1, results='hold'}


# importing the dataset as Adata, and making sure the data type for the variables are set properly for categorical variables.**

Adata1 <- data.frame(read.csv("acs2015_county_data.csv",header = TRUE))

str(Adata1)

#summary(Adata1)

#  we don't need to change any variables type, all of them set properly.
# There are a lot of variables , we may drop some of them, like :
#Hispanic, White,	Black,	Native,	Asian	Pacific, IncomeErr,	IncomePerCap,	IncomePerCapErr


# Dropping columns as agreed.  

Adata <- data.frame (subset(Adata1, select = -c(Hispanic,White,Black, Native, Asian, Pacific,IncomeErr, IncomePerCap, IncomePerCapErr, Drive, Carpool, Transit, Walk, OtherTransp, MeanCommute)))

str(Adata)
#summary(Adata)
  

```



**And here is the summary of our dataset **
```{r summary, results='hold'}


summary(Adata)


```


### Chapter 3: Data Cleaning.


As we did in the previous project, we performed data cleansing and normality tests to prepare our data for EDA. 

we performed the following steps:

1- Drop the unnecessary columns. 

2- Remove the outliers. 

3- Remove the Null values if exist.


Hint:To detect the outliers, we first polt it using Hitogram , then remove outliers, finally polt it again to check after removing the outliers. Same technique applied for all the 10 variables. 


Normality Test: 

We performed normality test in the previous project and here our conclusion:

we noticed that the columns that use rates instead of total number are normally distributed like: income, poverty,child poverty, unemployment, Production,WorkAtHome,PublicWork, SelfEmployed.While the columns that use total numbers are not normally distributed like: Total population and Employed columns. The columns that we need them for our tests are normally distributed, so we are good to go!

```{r step2, include=FALSE}

# After reading the data we need now to clean it to prepare for the analysis. 
# We started with "income", first we polte it to identify the outlier and then removing it. 
# Also , we removed the null values because it make error while do mathematics calculation. 
# Then we follow the same steps for the remaining columns that we will gonna use in this phase. 
# Currently our desired columns: TotalPop, Income, Poverty, ChildPoverty, Employed, Unemployment, Production, WorkAtHome,PublicWork, SelfEmployed

################################################################################


# First, lets take a look at the histogram for the income to detect the outliers 



hist(Adata$Income,
  xlab = "Income",
  main = "Histogram of Income",
  breaks = sqrt(nrow(Adata))
) # set number of bins




# # Now, we can extract the outliers from the incom using boxplot.stats()$out function.  
 income_out <- boxplot.stats(Adata$Income)$out
 income_out


# To count how many outliers in the income
 x <- data.frame(income_out)
x 
 nrow(x) # outliers numbers for income


 #Now, we will remove the outliers from our dataset and store the clean result in Adata2

  Adata2<- Adata[-which(Adata$Income %in% income_out),] # here we removed income outlier


 Adata2
 
 hist(Adata2$Income,
  xlab = "Income",
  main = "Histogram of Income",
  breaks = sqrt(nrow(Adata2))
) # set number of bins

 
# #Adata
# Adata2 


 # Now we will remove the naull values from the incmoe to allow for math calculations. 
 
 my.null <- data.frame (subset(Adata2,subset= is.na(Adata2$Income)))# To indicate the null values of income. 
my.null


Adata3 <- data.frame (subset(Adata2, subset= CensusId!= 48301 )) # To remove the record that contains null value

# Now we can calculate the max, mean , min .. etc of the income
max(Adata3$Income)
mean(Adata3$Income)
min(Adata3$Income)



```



```{r step3, include=FALSE}

# same thing for population column. clean the data. 

# First, lets take a look at the histogram for the population to detect the outliers 



hist(Adata3$TotalPop,
  xlab = "TotalPop",
  main = "Histogram of Population",
  breaks = sqrt(nrow(Adata3))
) # set number of bins




# # Now, we can extract the outliers from the population using boxplot.stats()$out function.  
 pop_out <- boxplot.stats(Adata3$TotalPop)$out
 pop_out


# To count how many outliers in the population
 x2 <- data.frame(pop_out)
x2 
 nrow(x2) # outliers numbers 


 #Now, we will remove the outliers from our dataset and store the clean result in Adata4

  Adata4<- Adata3[-which(Adata3$TotalPop %in% pop_out),] # here we removed  outlier


 Adata4
 
 hist(Adata4$TotalPop,
  xlab = "TotalPop",
  main = "Histogram of Population",
  breaks = sqrt(nrow(Adata4))
) # set number of bins

 
# #Adata
# Adata2 


 # Now we will remove the naull values from the population to allow for math calculations. 
 
 my.null2 <- data.frame (subset(Adata4,subset= is.na(Adata4$TotalPop)))# To indicate the null values of Population 
my.null2  # No null values. 


# Now we can calculate the max and the mean .. etc of the population. 
max(Adata4$TotalPop)
mean(Adata4$TotalPop)
min(Adata4$TotalPop)



```


```{r step4, include=FALSE}


# same thing for poverty column. clean the data. 

# First, lets take a look at the histogram for the poverty to detect the outliers 



hist(Adata4$Poverty,
  xlab = "Poverty",
  main = "Histogram of Poverty",
  breaks = sqrt(nrow(Adata4))
) # set number of bins




# # Now, we can extract the outliers from the population using boxplot.stats()$out function.  
 Poverty_out <- boxplot.stats(Adata4$Poverty)$out
 Poverty_out


# To count how many outliers in the population
 x3 <- data.frame(Poverty_out)
x3 
 nrow(x3) # outliers numbers 


 #Now, we will remove the outliers from our dataset and store the clean result in Adata5

  Adata5<- Adata4[-which(Adata4$Poverty %in% Poverty_out),] # here we removed  outlier


 Adata5
 
 hist(Adata5$Poverty,
  xlab = "Poverty",
  main = "Histogram of Poverty",
  breaks = sqrt(nrow(Adata5))
) # set number of bins

 
 



 # No null values. 
 

# Now we can calculate the max and the mean .. etc of the population. 
max(Adata4$Poverty)
mean(Adata4$Poverty)
min (Adata4$Poverty)






```



```{r step5, include=FALSE}


# same thing for Child Poverty column. clean the data. 

# First, lets take a look at the histogram for the Child Poverty to detect the outliers 



hist(Adata5$ChildPoverty,
  xlab = "ChildPoverty",
  main = "Histogram of ChildPoverty",
  breaks = sqrt(nrow(Adata5))
) # set number of bins




# # Now, we can extract the outliers using boxplot.stats()$out function.  
 ChildPoverty_out <- boxplot.stats(Adata5$ChildPoverty)$out
 ChildPoverty_out


# To count how many outliers in the population
 x4 <- data.frame(ChildPoverty_out)
x4
 nrow(x4) # outliers numbers 


 #Now, we will remove the outliers from our dataset and store the clean result in Adata6

  Adata6<- Adata5[-which(Adata5$ChildPoverty %in% ChildPoverty_out),] # here we removed  outlier


 Adata6
 
 hist(Adata6$ChildPoverty,
  xlab = "ChildPoverty",
  main = "Histogram of Child Poverty",
  breaks = sqrt(nrow(Adata6))
) # set number of bins

 
 

 
 my.null3 <- data.frame (subset(Adata6,subset= is.na(Adata6$ChildPoverty)))# To indicate the null values
my.null3  
 

 Adata7 <- data.frame (subset(Adata6, subset= CensusId!= 15005 )) # To remove the record that contains null value



# Now we can calculate the max and the mean .. etc of the population. 
max(Adata7$ChildPoverty)
mean(Adata7$ChildPoverty)
min (Adata7$ChildPoverty)






```




```{r step6, include=FALSE}

# same thing for Employed column. clean the data. 

# First, lets take a look at the histogram for the Employed to detect the outliers 



hist(Adata7$Employed,
  xlab = "Employed",
  main = "Histogram of Employed",
  breaks = sqrt(nrow(Adata7))
) # set number of bins




# # Now, we can extract the outliers using boxplot.stats()$out function.  
 Employed_out <- boxplot.stats(Adata7$Employed)$out
 Employed_out


# To count how many outliers in the population
 x5 <- data.frame(Employed_out)
x5
 nrow(x5) # outliers numbers 


 #Now, we will remove the outliers from our dataset and store the clean result in Adata8

  Adata8<- Adata7[-which(Adata7$Employed %in% Employed_out),] # here we removed outlier


 Adata8
 
 hist(Adata8$Employed,
  xlab = "Employed",
  main = "Histogram of Employed",
  breaks = sqrt(nrow(Adata8))
) # set number of bins

 
 
# No null values. 
 

# # Now we can calculate the max and the mean .. etc of the population. 
max(Adata8$Employed)
mean(Adata8$Employed)
min (Adata8$Employed)



```




```{r step7, include=FALSE}


# same thing for Unemployment column. clean the data. 

# First, lets take a look at the histogram for the Unemployment to detect the outliers 



hist(Adata8$Unemployment,
  xlab = "Unemployment",
  main = "Histogram of Unemployment",
  breaks = sqrt(nrow(Adata8))
) # set number of bins




 # Now, we can extract the outliers using boxplot.stats()$out function.  
 Unemployment_out <- boxplot.stats(Adata8$Unemployment)$out
 Unemployment_out
# 
# 
#  To count how many outliers in the population
  x6 <- data.frame(Unemployment_out)
 x6
  nrow(x6) # outliers numbers 
# 
# 
 #Now, we will remove the outliers from our dataset and store the clean result in Adata9
# 
   Adata10<- Adata8[-which(Adata8$Unemploymen %in% Unemployment_out),] # here we removed outlier
# 
# 
 Adata10
#  
  hist(Adata10$Unemployment,
   xlab = "Unemployment",
   main = "Histogram of Unemployment",
   breaks = sqrt(nrow(Adata10))
) # set number of bins
  


# # No null values. 
 
# 
# # # Now we can calculate the max and the mean .. etc . 
 max(Adata10$Unemployment)
 mean(Adata10$Unemployment)
min (Adata10$Unemployment)
 



```




```{r step 8, include=FALSE}


# same thing for Production column. clean the data. 

# First, lets take a look at the histogram for the Production to detect the outliers 



hist(Adata10$Production,
  xlab = "Production",
  main = "Histogram of Production",
  breaks = sqrt(nrow(Adata10))
) # set number of bins




 # Now, we can extract the outliers using boxplot.stats()$out function.  
 production_out <- boxplot.stats(Adata10$Production)$out
 production_out
# 
# 
#  To count how many outliers in the population
  x7 <- data.frame(production_out)
 x7
  nrow(x7) # outliers numbers 
# 
# 
 #Now, we will remove the outliers from our dataset and store the clean result in Adata11
# 
   Adata11<- Adata10[-which(Adata10$Production %in% production_out),] # here we removed outlier
# 
# 
 Adata11
#  
  hist(Adata11$Production,
   xlab = "Production",
   main = "Histogram of Production",
   breaks = sqrt(nrow(Adata11))
) # set number of bins
  


# # No null values. 
 
# 
# # # Now we can calculate the max and the mean .. etc . 
 max(Adata11$Production)
 mean(Adata11$Production)
min (Adata11$Production)
 






```

 

```{r step 9, include=FALSE}


# same thing for WorkAtHome column. clean the data. 

# First, lets take a look at the histogram for the WorkAtHome to detect the outliers 



hist(Adata11$WorkAtHome,
  xlab = "WorkAtHome",
  main = "Histogram of WorkAtHome",
  breaks = sqrt(nrow(Adata11))
) # set number of bins




 # Now, we can extract the outliers using boxplot.stats()$out function.  
 WorkAtHome_out <- boxplot.stats(Adata11$WorkAtHome)$out
 WorkAtHome_out
# 
# 
#  To count how many outliers in the population
  x8 <- data.frame(WorkAtHome_out)
 x8
  nrow(x8) # outliers numbers 
# 
# 
 #Now, we will remove the outliers from our dataset and store the clean result in Adata11
# 
   Adata12<- Adata11[-which(Adata11$WorkAtHome %in% WorkAtHome_out),] # here we removed outlier
# 
# 
 Adata12
#  
  hist(Adata12$WorkAtHome,
   xlab = "WorkAtHome",
   main = "Histogram of WorkAtHome",
   breaks = sqrt(nrow(Adata12))
) # set number of bins
  


# # No null values. 
 
# 
# # # Now we can calculate the max and the mean .. etc . 
 max(Adata12$WorkAtHome)
 mean(Adata12$WorkAtHome)
min (Adata12$WorkAtHome)
 






```




```{r step10, include=FALSE}


# same thing for PublicWork column. clean the data. 

# First, lets take a look at the histogram for the PublicWork to detect the outliers 



hist(Adata12$PublicWork,
  xlab = "PublicWork",
  main = "Histogram of PublicWork",
  breaks = sqrt(nrow(Adata12))
) # set number of bins




 # Now, we can extract the outliers using boxplot.stats()$out function.  
 PublicWork_out <- boxplot.stats(Adata12$PublicWork)$out
 PublicWork_out
# 
# 
#  To count how many outliers in the population
  x9 <- data.frame(PublicWork_out)
 x9
  nrow(x9) # outliers numbers 
# 
# 
 #Now, we will remove the outliers from our dataset and store the clean result in Adata11
# 
   Adata13<- Adata12[-which(Adata12$PublicWork %in% PublicWork_out),] # here we removed outlier
# 
# 
 Adata13
#  
  hist(Adata13$PublicWork,
   xlab = "PublicWork",
   main = "Histogram of PublicWork",
   breaks = sqrt(nrow(Adata13))
) # set number of bins
  


# # No null values. 
 
# 
# # # Now we can calculate the max and the mean .. etc . 
 max(Adata13$PublicWork)
 mean(Adata13$PublicWork)
min (Adata13$PublicWork)
 



```





```{r step11, include=FALSE}

# same thing for PublicWork column. clean the data. 

# First, lets take a look at the histogram for the PublicWork to detect the outliers 



hist(Adata13$SelfEmployed,
  xlab = "SelfEmployed",
  main = "Histogram of SelfEmployed",
  breaks = sqrt(nrow(Adata13))
) # set number of bins




 # Now, we can extract the outliers using boxplot.stats()$out function.  
 SelfEmployed_out <- boxplot.stats(Adata13$SelfEmployed)$out
 SelfEmployed_out
# 
# 
#  To count how many outliers in the population
  x10 <- data.frame(SelfEmployed_out)
 x10
  nrow(x10) # outliers numbers 
# 
# 
 #Now, we will remove the outliers from our dataset and store the clean result in Adata11
# 
   Adata14<- Adata13[-which(Adata13$SelfEmployed %in% SelfEmployed_out),] # here we removed outlier
# 
# 
 Adata14
#  
  hist(Adata14$SelfEmployed,
   xlab = "SelfEmployed",
   main = "Histogram of SelfEmployed",
   breaks = sqrt(nrow(Adata14))
) # set number of bins
  


# # No null values. 
 
# 
# # # Now we can calculate the max and the mean .. etc . 
 max(Adata14$SelfEmployed)
 mean(Adata14$SelfEmployed)
min (Adata14$SelfEmployed)





```



**Final dataset **

As we can see bellow, after performing the cleansing process, we removed 1,147 record which include 2 NA values and 1,145 outliers. Now we have 2073 obs across 22 variables, the data shrink, however we have now more accurate dataset that will help us perform reliable tests as well as more accurate results.   



```{r final dataset, results='hold'}

# This is the final data set:
Adata9 = Adata14

str(Adata9)

#summary(Adata9)

#########################




```


**Final Dataset Summary **
```{r final summary, results='hold'}

summary(Adata9)


```


```{r SD, include=FALSE}


# Standard deviation for the columns: 

sd(Adata9$Income)
sd(Adata9$TotalPop)
sd(Adata9$Poverty)
sd(Adata9$ChildPoverty)
sd(Adata9$Employed)
sd(Adata9$Unemployment)
sd(Adata9$Production)
sd(Adata9$WorkAtHome)
sd(Adata9$PublicWork)
sd(Adata9$SelfEmployed)

```

### Chapter 4: Income Prediction Test.

**How can we predict the average income in counties?**

First, we will use correlation matrix to see the relationships between different variables.

```{r income, results='hold'}

loadPkg("lattice")
loadPkg("ggplot2")
loadPkg("corrplot")




# To subset the data with numerical values only. 
data_num2 <- data.frame (subset(Adata9, select = -c(State,County,CensusId,Citizen,Service,Office,Professional, Construction, Production,WorkAtHome, TotalPop, Men, Women, FamilyWork)))


matrix2 = cor(data_num2) # get the correlation matrix between all numerical variables.
head(matrix2)
matrix2

corrplot(matrix2)


corrplot(matrix2, method = "number", type="upper")



```


**By using numerical variables only, we will build a linear model with 1 independent variable to predict the income.**  

We Choose a variable with strong correlation coefficient and here is our result:

1- Multiple R-squared value is equal to: 0.3 which means:
3% of the variability in the income is explained by the variability in Unemployment. 

2- The p-value is less than 0.05 which means the Unemployment is statistically significant predictor of income. 

3- The coefficient estimate value is equals to -1575.3 which means that there is 1575.3 decrease in the income when the Unemployment changed by 1. 

4- Coefficient-Standard Error= 52.9, so we can say that the income can vary by 52.9



```{r incomeCore, results='hold'}

cor.test(Adata9$Unemployment, Adata9$Income) -> cortest_1
cortest_1

fit_1 <- lm(Income ~ Unemployment, data = Adata9)
summary(fit_1)

xkabledply(fit_1, title = paste("Model :", format(formula(fit_1)) ) )

plot(fit_1)

```


**Next, we will add a second variable to the model.**  

We Choose a variable with stronger correlation coefficient and here is our result:

1- Multiple R-squared value is equal to: 0.675 which means:
67.5% of the variability in income is explained by the variability in unemployment and poverty.

2- The p-values of unemployment and poverty are less than 0.05 which means that they are statistically significant predictor of income. 

3- The coefficient estimate value of the unemployment is equals to -186.2 which means that there is 186.2 decrease in the income when the unemployment changed by 1. 

4- The coefficient estimate value of the poverty is equals to -1220.5 which means that there is 1220.5 decrease in the income when the poverty changed by 1. 

5- The Coefficient-Standard Error for the unemployment = 45.9, so we can say that the income can vary by 45.9

6- The Coefficient-Standard Error for the poverty = 25, so we can say that the income can vary by 25
```{r secondvarible, results='hold'}

fit_2 <- lm(Income ~ Unemployment+Poverty, data = Adata9)
summary(fit_2)




xkabledply(fit_2, title = paste("Model :", format(formula(fit_2)) ) )
xkablevif(fit_2)


```


**We will try one more time to add a third variable in our model.**


```{r thirdvarible, results='hold'}

fit_3 <- lm(Income ~ Unemployment+Poverty+ChildPoverty, data = Adata9)
summary(fit_3)



xkabledply(fit_3, title = paste("Model :", format(formula(fit_3)) ) )
xkablevif(fit_3)


```
**Confidence intervals of the coefficients.**  

```{r co3, results='hold'}

confint(fit_3)


```

**We used ANOVA to compare the three different models we found.**  

----------------------------------------------------------------------------------------------------------------

The p-value for the second model < 0.05 , which means that the second model is significantly different than the first model. 

The p-value for the third model is not that significant. 

As a result, the second model is the best among all. 


```{r anova11,results='hold'}

anovaRes_1 <- anova(fit_1,fit_2,fit_3) 
anovaRes_1
str(anovaRes_1)

xkabledply(anovaRes_1, title = "ANOVA comparison between the models") # to show it in a table

```

### Chapter 5: Unemployment Prediction Test.

1.	Does a relationship exist between ‘child poverty’ and ‘unemployment’?

First, let's start to get a general idea of how ChildPoverty relates to all the factors by looking at the correlation matrix between 'ChildPoverty' and the other variables.
 1. Correlation
 2.EDA--Exploratory Data Analysis (EDA) is the process of analyzing and visualizing the data to get a better          understanding of the data and glean insight from it. There are various steps involved when doing EDA but the         following are the common steps that a data analyst can take when performing EDA:
 a.Import the data
 b.Clean the data
 c.Process the data
 d.Visualize the data
 3.Regression-----Regression analysis is a group of statistical processes used in R programming and statistics to     determine the relationship between dataset variables. Generally, regression analysis is used to determine the        relationship between the dependent and independent variables of the dataset.
 4.Decision Tree plotting

```{r Sagar}

library(corrplot)
subset1<-subset(Adata9,select = -c(CensusId,State,County))
crr<-cor(subset1)
corrplot(crr,method = 'number',type = 'upper',number.font = 1, number.cex = 0.5)


# EDA on child poverty
library(ggplot2)
library(ISLR2)
data(package="ISLR2")
require(tree)
Adata10 <- Adata9
names(Adata10)

#histogram of child Poverty and unemployement

hist(Adata10$ChildPoverty, main="Child poverty", xlab = "Child poverty",col = c("blue", "red", "gray", "green"))

hist(Adata10$Unemployment, main="Unemployment",xlab = "Unemployment" ,col = c("blue", "red", "gray", "green"))

# regression and scatter plot of child poverty and Selected features.
  
ggplt <- ggplot(Adata9,aes(x=ChildPoverty,y=Unemployment))+
         geom_point()+labs(title = "ChildPoverty vs Unemployment")+labs(x="ChildPoverty", y="Unemployment" )+
         theme_classic()
  
ggplt
  
# Plotting a single Regression Line
ggplt+geom_smooth(method=lm,se=FALSE,fullrange=TRUE)


  
ggplt1 <- ggplot(Adata9,aes(x=ChildPoverty,y=Income))+
         geom_point()+labs(title = "ChildPoverty vs Income")+labs(x="ChildPoverty", y="Income" )+
         theme_classic()
  
ggplt1
  
# Plotting a single Regression Line
ggplt1+geom_smooth(method=lm,se=FALSE,fullrange=TRUE)


ggplt2 <- ggplot(Adata9,aes(x=ChildPoverty,y=Poverty))+
         geom_point()+labs(title = "ChildPoverty vs Poverty")+labs(x="ChildPoverty", y="Poverty" )+
         theme_classic()
  
ggplt2
  
# Plotting a single Regression Line
ggplt2+geom_smooth(method=lm,se=FALSE,fullrange=TRUE)



ggplt3 <- ggplot(Adata9,aes(x=ChildPoverty,y=Professional))+
         geom_point()+labs(title = "ChildPoverty vs Professional")+labs(x="ChildPoverty", y="Professional" )+
         theme_classic()
  
ggplt3
  
# Plotting a single Regression Line
ggplt3+geom_smooth(method=lm,se=FALSE,fullrange=TRUE)



ggplt3 <- ggplot(Adata9,aes(x=ChildPoverty,y=Professional))+
         geom_point()+labs(title = "ChildPoverty vs Professional")+labs(x="ChildPoverty", y="Professional" )+
         theme_classic()
  
ggplt3
  
# Plotting a single Regression Line
ggplt3+geom_smooth(method=lm,se=FALSE,fullrange=TRUE)


ggplt4 <- ggplot(Adata9,aes(x=ChildPoverty,y=Service))+
         geom_point()+labs(title = "ChildPoverty vs Service")+labs(x="ChildPoverty", y="Service" )+
         theme_classic()
  
ggplt4
  
# Plotting a single Regression Line
ggplt4+geom_smooth(method=lm,se=FALSE,fullrange=TRUE)


ggplt5 <- ggplot(Adata9,aes(x=ChildPoverty,y=Production))+
         geom_point()+labs(title = "ChildPoverty vs Production")+labs(x="ChildPoverty", y="Production" )+
         theme_classic()
  
ggplt5
  
# Plotting a single Regression Line
ggplt5+geom_smooth(method=lm,se=FALSE,fullrange=TRUE)


ggplt6 <- ggplot(Adata9,aes(x=ChildPoverty,y=WorkAtHome))+
         geom_point()+labs(title = "ChildPoverty vs WorkAtHome")+labs(x="ChildPoverty", y="WorkAtHome" )+
         theme_classic()
  
ggplt6
  
# Plotting a single Regression Line
ggplt6+geom_smooth(method=lm,se=FALSE,fullrange=TRUE)



ggplt7 <- ggplot(Adata9,aes(x=ChildPoverty,y=PublicWork))+
         geom_point()+labs(title = "ChildPoverty vs PublicWork")+labs(x="ChildPoverty", y="PublicWork" )+
         theme_classic()
  
ggplt7
  
# Plotting a single Regression Line
ggplt7+geom_smooth(method=lm,se=FALSE,fullrange=TRUE)


ggplt8 <- ggplot(Adata9,aes(x=ChildPoverty,y=SelfEmployed))+
         geom_point()+labs(title = "ChildPoverty vs SelfEmployed")+labs(x="ChildPoverty", y="SelfEmployed" )+
         theme_classic()
  
ggplt8
  
# Plotting a single Regression Line
ggplt8+geom_smooth(method=lm,se=FALSE,fullrange=TRUE)


#Keep in mind that child poverty is a quantitative variable. I would like to show this using a tree of binary responses. To do this, convert ChildPoverty to a binary variable called High. If your childpoverty is less than 20, 
lm_p<-glm(ChildPoverty~Unemployment,data=Adata9)
summary(lm_p)
plot(lm_p)

pred_p<-predict(lm_p,data.frame(Unemployment=Adata9$Unemployment))
mse_p<-mean((pred_p-Adata9$ChildPoverty)^2)
mse_p

subset1<-subset(Adata9,select = -c(CensusId,State,County))
lm1<-glm(ChildPoverty~.,data=subset1)
summary(lm1)
par(mfrow=c(2,2))
plot(lm1)

subset1<-subset(subset1,select = -Women)
lm2<-glm(ChildPoverty~.,data=subset1)
summary(lm2)
par(mfrow=c(2,2))
plot(lm2)

library(FNN)
set.seed(1) ## the seed can be arbitrary but we use 1 for the sake of consistency
fold.index <- cut(sample(1:nrow(subset1)),breaks=10,labels=FALSE)
mse_l<-rep(0,10)
error_l<-rep()
for (i in 1:10) {
  fit_l<-glm(log(Income)~.,data=subset1[fold.index!=i,])
  result_l<-exp(predict(fit_l,subset1[fold.index==i,]))
  mse_l[i]<-mean((result_l-subset1[fold.index==i,5])^2)
}
mse_l<-mean(mse_l)
mse_l
lm3<-lm(log(Income)~TotalPop+Men+Citizen+Poverty+ChildPoverty+WorkAtHome+Employed+Unemployment,data=subset1)
summary(lm3)
plot(lm3)



```

If you look at the correlation diagram, you can see that the variables are "income", "poverty", "Professional", "service", "production", "WorkAtHome", "public works", "selfemployed", and "unemployment". .. 'Has a correspondingly high correlation. So I would like to run some tests to see if they really are  the factors that influence child poverty. 

As long as all the above variables  are checked for normal distribution in Chapter 3, you can run tests directly on them. The independent and dependent variables "child poverty" are all quantitative, so ttest is the best choice for testing hypotheses.

Next, run two sample tests to see if the poverty levels in the counties with different incomes are different. To set it up, divide the data into two parts (one above average income and one below average income), get a sample of size 50, and $ \ alpha. Set the $ value. Up to 0.05 and our hypothesis is:

$ H_0 $: $ \ mu_ {upper} = \ mu_ {lower} $, the average child poverty of the two partial datasets is the same.
$ H_1 $: $ \ mu_ {upper} \ neq \ mu_ {lower} $, the average child poverty of the two partial datasets is not  the same.


The MSE calculated from cross-validation is approximately the same but slightly smaller than the previous linear model. Thus, the linear model using our selected predictors might predict Child Poverty more precisely.

####################

```{r income test,echo=FALSE,results='hold'}

#Geereshmanajali part
set.seed(123)
income_upper<-Adata9[Adata9$Income>mean(Adata9$Income),]
income_lower<-Adata9[Adata9$Income<mean(Adata9$Income),]
iu_sample<-income_upper[sample(nrow(income_upper),50),]
il_sample<-income_lower[sample(nrow(income_lower),50),]
income_ttest<-t.test(iu_sample$ChildPoverty,il_sample$ChildPoverty)
income_ttest

```

Since the p-value we got from the t-test is $2*10^{-16}$ which is smaller than a $\alpha$ value of 0.05, we have sufficient evidence to reject $H_0$ and conclude that there is a difference between the mean ChildPoverty for the two groups. Thus, 'ChildPoverty' is affected by 'Income'.


```{r Poverty test,echo=FALSE,results='hold'}

set.seed(123)
UE_upper<-Adata9[Adata9$Poverty>mean(Adata9$Poverty),]
UE_lower<-Adata9[Adata9$Poverty<mean(Adata9$Poverty),]
pov_sample<-UE_upper[sample(nrow(UE_upper),50),]
pov_sample<-UE_lower[sample(nrow(UE_lower),50),]
UE_ttest<-t.test(pov_sample$ChildPoverty,pov_sample$ChildPoverty)
UE_ttest

```

P-value for 'Poverty' is $2*10^{-4}$ which is less than 0.05, we reject the null and state that 'Poverty' is a affecting factor on 'ChildPoverty'.

```{r professional test,echo=FALSE,results='hold'}

set.seed(123)
prof_upper<-Adata9[Adata9$Professional>mean(Adata9$Professional),]
prof_lower<-Adata9[Adata9$Professional<mean(Adata9$Professional),]
pu_sample<-prof_upper[sample(nrow(prof_upper),50),]
pl_sample<-prof_lower[sample(nrow(prof_lower),50),]
prof_ttest<-t.test(pu_sample$ChildPoverty,pl_sample$ChildPoverty)
prof_ttest


```

P-value for 'Professional' is $2*10^{-4}$ which is less than 0.05, we reject the null and state that 'Professional' is a affecting factor on 'ChildPoverty'.


```{r Service test,echo=FALSE,results='hold'}
set.seed(123)
serv_upper<-Adata9[Adata9$Service>mean(Adata9$Service),]
serv_lower<-Adata9[Adata9$Service<mean(Adata9$Service),]
su_sample<-serv_upper[sample(nrow(serv_upper),50),]
sl_sample<-serv_lower[sample(nrow(serv_lower),50),]
serv_ttest<-t.test(su_sample$ChildPoverty,sl_sample$ChildPoverty)
serv_ttest
```
P-value for 'Service' is 0.004 which is smaller than 0.05, we reject the null and state that 'Service' is an affecting factor for 'ChildPoverty'.

```{r production test,echo=FALSE,results='hold'}
set.seed(123)
prod_upper<-Adata9[Adata9$Production>mean(Adata9$Production),]
prod_lower<-Adata9[Adata9$Production<mean(Adata9$Production),]
produ_sample<-prod_upper[sample(nrow(prod_upper),50),]
prodl_sample<-prod_lower[sample(nrow(prod_lower),50),]
prod_ttest<-t.test(produ_sample$ChildPoverty,prodl_sample$ChildPoverty)
prod_ttest
```
P-value for 'Production' is 0.2 which is greater than 0.05, we can not reject $H_0$, and can not conclude that 'Production' is an affecting factors for 'ChildPoverty'.

```{r WorkAtHome test,echo=FALSE,results='hold'}
set.seed(123)
WAH_upper<-Adata9[Adata9$WorkAtHome>mean(Adata9$WorkAtHome),]
WAH_lower<-Adata9[Adata9$WorkAtHome<mean(Adata9$WorkAtHome),]
wu_sample<-WAH_upper[sample(nrow(WAH_upper),50),]
wl_sample<-WAH_lower[sample(nrow(WAH_lower),50),]
WAH_ttest<-t.test(wu_sample$ChildPoverty,wl_sample$ChildPoverty)
WAH_ttest
```
P-value for "WorkAtHome' is extremely small, we have sufficient evidence to state that "WorkAtHome' is an effecting factor for 'ChildPoverty'.

```{r PublicWork test,echo=FALSE,results='hold'}
set.seed(123)
PW_upper<-Adata9[Adata9$PublicWork>mean(Adata9$PublicWork),]
PW_lower<-Adata9[Adata9$PublicWork<mean(Adata9$PublicWork),]
pubu_sample<-PW_upper[sample(nrow(PW_upper),50),]
publ_sample<-PW_lower[sample(nrow(PW_lower),50),]
PW_ttest<-t.test(pubu_sample$ChildPoverty,publ_sample$ChildPoverty)
PW_ttest
```

P-value for 'PublicWork' is greater than 0.05, so it is not an effecting factor for 'Poverty'.
```{r SelfEmployed test,echo=FALSE,results='hold'}
set.seed(123)
SE_upper<-Adata9[Adata9$SelfEmployed>mean(Adata9$SelfEmployed),]
SE_lower<-Adata9[Adata9$SelfEmployed<mean(Adata9$SelfEmployed),]
semu_sample<-SE_upper[sample(nrow(SE_upper),50),]
seml_sample<-SE_lower[sample(nrow(SE_lower),50),]
SE_ttest<-t.test(semu_sample$ChildPoverty,seml_sample$ChildPoverty)
SE_ttest
```
P-value for 'SelfEmployment' is 0.1 which is greater than 0.05, we can not reject $H_0$,  so it is not an effecting factor for 'ChildPoverty'.

```{r UnEmployment test,echo=FALSE,results='hold'}
set.seed(123)
UE_upper<-Adata9[Adata9$Unemployment>mean(Adata9$Unemployment),]
UE_lower<-Adata9[Adata9$Unemployment<mean(Adata9$Unemployment),]
uemu_sample<-UE_upper[sample(nrow(UE_upper),50),]
ueml_sample<-UE_lower[sample(nrow(UE_lower),50),]
UE_ttest<-t.test(uemu_sample$ChildPoverty,ueml_sample$ChildPoverty)
UE_ttest
```

P-value for 'Unemployment' is $2*10^{-7}$, so we have sufficient evidence to reject null and conclude that it is an effecting factor for 'ChildPoverty'.

Noticing that 'State' is left out from our correlation matrix since it is a categorical variable, we want to perform an ANOVA test to see if there's difference in 'ChildPoverty' across different states.

```{r State ANOVA test,echo=FALSE,results='hold'}
anostate<-aov(ChildPoverty~State,data = Adata9)
summary(anostate)
```
As we can see in the results of ANOVA test, our p-value is extremely small, so we have sufficient evidence to reject the null and conclude that there is difference in 'ChildPoverty' for different states. In this case, 'State' is an effecting factor for 'Poverty'. Finally, we will perform a Post-hoc test to see the pairs of states that have different 'ChildPoverty' as below. We will not display the result here since we not need this information to answer our SMART question, just save it for further analysis if needed.

when you understand how each independent variable’s mean is different from the others, you can begin to understand which of them has a connection to your dependent variable (such as landing page clicks) and begin to learn what is driving that behavior.

You could also flip things around and see whether or not a single independent variable (such as temperature) affects multiple dependent variables (such as purchase rates of suncream, attendance at outdoor venues, and likelihood to hold a cook-out) and if so, which ones.

### Chapter 6: Professional and Income Test.

First, we plotted histograms for both Professional and Income to see their distributions. Since the two histograms are approximately bell shaped (normally distributed), we can use them directly for modeling.

```{r hists,echo=FALSE,results='hold'}
hist(Adata9$Professional)
hist(Adata9$Income)
```

We will also plot a scatter plot to see the relationship between Professional and Income. The plot appears to have a pretty obvious linear pattern where Income increases as Professional increases. So we would like to fit a linear model to predict Income with Professional.

```{r incom vs prof,echo=FALSE,results='hold'}
plot(Adata9$Professional,Adata9$Income)
```
From the statistic summary of our model, we can see that in this single predictor model, Professional turns out to be a really significant factor impacting Income. For each 1 percent increase in population with professional jobs in each state, the average state income increases by 884.4.


Interpretations on the plots for our model:

The dots in Residuals vs Fitted plot are evenly spread out above and below our fitted line and the line is approximately horizontal, meaning that the relationship is really close to linear as how to fit the model. The Q-Q plot has no obvious curving and long-tailed diagnostics, which also indicate normality.

There are no data points with extreme standard deviance residuals meaning that there are no outliers. Furthermore, there are no specific data points with a large Cook's distance, so there's no high leverage points in our model.

```{r lm prof plot,echo=FALSE,results='hold'}
lm_p<-glm(Income~Professional,data=Adata9)
summary(lm_p)
plot(lm_p)
pred_p<-predict(lm_p,newdata=data.frame(Professional=Adata9$Professional))
mean((pred_p-Adata9$Income)^2)
```

As we can see from the prediction, the mean squared error is pretty large. So can we fit a better model to predict Income with all the variables? What model and factors we should use to minimize the prediction error? Let's use cross-validation to estimate the error for each model.

```{r lm prof mse,echo=FALSE,results='hold'}
pred_p<-predict(lm_p,data.frame(Professional=Adata9$Professional))
mse_p<-mean((pred_p-Adata9$Income)^2)
mse_p
```

*Linear Regression*

Fitting model with all predictors:

```{r lm1 plot,echo=FALSE,results='hold'}
subset1<-subset(Adata9,select = -c(CensusId,State,County))
lm1<-glm(Income~.,data=subset1)
summary(lm1)
par(mfrow=c(2,2))
plot(lm1)
```

As we can see form the summary, statistics for the variable "Women" are NAs, meaning that Women is probably a linear dependent pair with "Men". So let's drop this predictor.

```{r lm1 drop predictor,echo=FALSE,results='hold'}
subset1<-subset(subset1,select = -Women)
lm2<-glm(Income~.,data=subset1)
summary(lm2)
par(mfrow=c(2,2))
plot(lm2)
```

According to the Residual vs Fitted plot, the fitted line is curved, so we are going to try some transformation on y to see if it will fix the non-linearity.

```{r lm2 plot,echo=FALSE,results='hold'}
lm2<-glm(log(Income)~.,data=subset1)
summary(lm2)
par(mfrow=c(2,2))
plot(lm2)
```

*CV error estimating for Linear model with all predictors*

```{r CV lm2,echo=FALSE,results='hold'}
library(FNN)
set.seed(1) ## the seed can be arbitrary but we use 1 for the sake of consistency
fold.index <- cut(sample(1:nrow(subset1)),breaks=10,labels=FALSE)
mse_l<-rep(0,10)
error_l<-rep()
for (i in 1:10) {
  fit_l<-glm(log(Income)~.,data=subset1[fold.index!=i,])
  result_l<-exp(predict(fit_l,subset1[fold.index==i,]))
  mse_l[i]<-mean((result_l-subset1[fold.index==i,5])^2)
}
mse_l<-mean(mse_l)
mse_l
```

Based on the summary for our previous linear model, what if we use only the significant predictors with a p-value < 0.05 to model Income? 

```{r lm3 plots,echo=FALSE,results='hold'}
lm3<-lm(log(Income)~TotalPop+Men+Citizen+Poverty+ChildPoverty+WorkAtHome+Employed+Unemployment,data=subset1)
summary(lm3)
plot(lm3)
```
The statistical summary for this model is shown above. Now let's compare the CV error.

```{r CV lm3,echo=FALSE,results='hold'}
set.seed(1) ## the seed can be arbitrary but we use 1 for the sake of consistency
fold.index <- cut(sample(1:nrow(subset1)),breaks=10,labels=FALSE)
mse_l<-rep(0,10)
error_l<-rep()
for (i in 1:10) {
  fit_l<-glm(log(Income)~Citizen+Poverty+ChildPoverty+WorkAtHome+Employed+Unemployment,data=subset1[fold.index!=i,])
  result_l<-exp(predict(fit_l,subset1[fold.index==i,]))
  mse_l[i]<-mean((result_l-subset1[fold.index==i,5])^2)
}
mse_l<-mean(mse_l)
mse_l
```


The MSE calculated from cross-validation is approximately the same but slightly smaller than the previous linear model. Thus, the linear model using our selected predictors might predict Income more precisely.


*Subset Selection*

Noticing the fact that we have a relevantly large data set with a large number of rows, as well as 17 predictors, we choose to use backward-stepwise subset selection to do model selection. 

```{r backward plots,echo=FALSE,results='hold'}
library(leaps)
regfit.bwd <- regsubsets( Income~., data = subset1, nvmax = 17, method = "backward")
reg.bwd.summary <- summary(regfit.bwd)
plot(reg.bwd.summary$rss ,xlab="Number of Variables ", ylab="RSS", type="l")
plot(reg.bwd.summary$adjr2 ,xlab="Number of Variables ", ylab="Adjusted RSq", type="l")
bestadjr2<-which.max(reg.bwd.summary$adjr2)
points(bestadjr2, reg.bwd.summary$adjr2[bestadjr2], col = "red", cex = 2, pch = 20)
plot(reg.bwd.summary$cp, xlab = "Number of Variables ", ylab = "Cp", type = "l")
bestcp<-which.min(reg.bwd.summary$cp)
points(bestcp,reg.bwd.summary$cp[bestcp], col = "red", cex = 2, pch = 20)
bestbic<-which.min(reg.bwd.summary$bic)
plot(reg.bwd.summary$bic, xlab = "Number of Variables ", ylab = "BIC", type = "l")
points(bestbic,reg.bwd.summary$bic[bestbic], col = "red", cex = 2, pch = 20)
bestadjr2
bestcp
bestbic
```

We will first calculate the CV MSE with a subset of our own choice. Since we want to maximize the Adjusted R-squared value as well as minimize Cp and BIC values. We choose an id of 12. And the MSE for modeling with id=12 is 17159885.

```{r 12 subset CV,echo=FALSE,results='hold'}
predict.regsubsets <- function (object, newdata , id, ...){
  form <- as.formula(object$call[[2]]) 
  mat <- model.matrix(form, newdata)    
  coefi <- coef(object, id = id)       
  xvars <- names(coefi)           
  return(mat[,xvars] %*% coefi)    
}

set.seed(1)
fold.index <- cut(sample(1:nrow(subset1)), breaks=10, labels=FALSE)

error <- rep(0, 10)
for (k in 1:10){
  best.fit <- regsubsets(Income ~ ., data = subset1[fold.index!=k,], nvmax = 17)
  pred <- predict(best.fit,subset1[fold.index==k,], id = 12)
  error[k] <- mean((pred - subset1[fold.index==k,][,'Income'])^2)
}
error_ms <- mean(error)
error_ms
```

Now we loop through different ids to see the MSE for the best subset selected by CV method.

```{r CV tuning,echo=FALSE,results='hold'}
set.seed(1)
fold.index <- cut(sample(1:nrow(subset1)), breaks=10, labels=FALSE)
error_ms <- rep(0,17)
for(i in 1:17){  
  error <- rep(0, 10)
  for (k in 1:10){
    best.fit <- regsubsets(Income ~ ., data = subset1[fold.index!=k,], nvmax = 17)
    pred <- predict(best.fit,subset1[fold.index==k,], id = i)
    error[k] <- mean((pred - subset1[fold.index==k,][,'Income'])^2)
  }
  error_ms[i] <- mean(error)
}
bestcv<-which.min(error_ms)
bestcv
error_ms<-error_ms[bestcv]
error_ms
```

The model with 15 variables turns out to be the best with a MSE of 1.708*10^7

*Principal Component Regression*

Now let's use the pcr function to fit a model with 10-fold cross validation to see how this model performs.

```{r PCR,echo=FALSE,results='hold'}
library("pls")
fit.pcr <- pcr(Income~. ,data=subset1,scale=TRUE,validation="CV")
validationplot(fit.pcr,val.type="MSEP",main="MSE for n component by 10-fold CV")

```

Reading from the plot, we can see that the lowest MSE appears when we use 15 component, which is approximately 2e+07.

*Shrinkage Methods*

*Ridge regression*: 

Last but not least, we also use the shrinkage method to fit both ridge regression and lasso regression.

```{r Ridge Regression,echo=FALSE,results='hold'}
library(glmnet)
set.seed(1)
fit_ridge<-glmnet(model.matrix(Income~.,data=subset1),subset1[,5], alpha = 0)
ridge_out <- cv.glmnet(model.matrix(Income~.,data=subset1),subset1[,5], alpha = 0, nfolds = 10)
plot(ridge_out)
bestlam_ridge <- ridge_out$lambda.min
bestlam_ridge
prediction_ridge <- predict(fit_ridge, s = bestlam_ridge, newx =model.matrix(Income~.,subset1) )
mean((prediction_ridge-subset1$Income)^2)
```

We set alpha equals to 0 and we can see that the MSE for the prediction increases slightly from 0 to 2, then the increment accelerates from 2 to 5, and finally slows down from 5 to 8. 
From the CV, we get our best lambda value of 0.558. The MSE for ridge regression model with $\lambda$=0.558 is 2.03e+09

*Lasso Regression*

```{r Lasso Regression,echo=FALSE,results='hold'}
fit_lasso<-glmnet(model.matrix(Income~.,data=subset1),subset1[,5], alpha = 1)
set.seed(1)
lasso_out <- cv.glmnet(model.matrix(Income~.,data=subset1),subset1[,5], alpha = 1,nfolds = 10)
plot(lasso_out)
bestlam_lasso <- lasso_out$lambda.min
bestlam_lasso
prediction_lasso<- predict(fit_lasso, s = bestlam_lasso, newx =model.matrix(Income~.,subset1) )
mean((prediction_lasso-subset1$Income)^2)
```

The plot shows a slight increase of MSE as log lambda increases until it reaches 0.5, then the increment accelerates for larger log values.
From the CV, we get our best lambda value of 0.163, and the MSE for lasso regression model with $\lambda$=0.163 is 2.03e+09.


*Conclusion*

By comparing the test MSE for all the models we build, backward-stepwise subset selection with 15 variables performs the best, Principal Component Regression has the second smallest MSE, linear regression and Shrinkage methods have a relevantly large MSE.The MSE calculated from cross-validation is approximately the same but slightly smaller than the previous linear model. Thus, the linear model using our selected predictors might predict Child poverty more precisely.





